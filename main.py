"""
Streamlit ê³ ê° ë“±ê¸‰(í´ëŸ¬ìŠ¤í„°) ì˜ˆì¸¡ ì•± - ê°œì„  ë²„ì „
- í•œêµ­ì–´ ì»¬ëŸ¼ëª… ì§€ì›
- íƒ­ ê¸°ë°˜ UIë¡œ ì§ê´€ì ì¸ ê²½í—˜ ì œê³µ
- í•™ìŠµ â†’ ì˜ˆì¸¡ â†’ ì‹œê°í™” ë¶„ë¦¬
- ì‹¤ì‹œê°„ ì§„í–‰ë¥  í‘œì‹œ ë° ìƒì„¸ í”¼ë“œë°±
"""
from __future__ import annotations

import os
from typing import Dict, Tuple
import pickle
from datetime import datetime

import numpy as np
import pandas as pd
import streamlit as st
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from scipy.stats import uniform, randint
from lightgbm import LGBMClassifier
import altair as alt

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„¤ì • ë° ìƒìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SEED: int = 42
CLUSTER_K: int = 4

st.set_page_config(
    page_title="ê³ ê° ë“±ê¸‰ ì˜ˆì¸¡ ì‹œìŠ¤í…œ",
    layout="wide",
    initial_sidebar_state="expanded"
)
alt.themes.enable("opaque")

# í•œêµ­ì–´ ì»¬ëŸ¼ ë§¤í•‘
COLUMN_NAME_KR = {
    "annual_income": "ì—°ì†Œë“(ì›)",
    "Recency": "ìµœê·¼êµ¬ë§¤ê²½ê³¼ì¼(ì¼)",
    "Monetary": "ì´êµ¬ë§¤í•©ì‚°ì•¡(ì›)",
    "Frequency": "ì´êµ¬ë§¤ë¹ˆë„(íšŒ)",
    "num_purchase_store": "ë§¤ì¥êµ¬ë§¤íšŸìˆ˜(íšŒ)",
    "num_purchase_web": "ì˜¨ë¼ì¸êµ¬ë§¤íšŸìˆ˜(íšŒ)",
    "num_purchase_discount": "í• ì¸êµ¬ë§¤íšŸìˆ˜(íšŒ)",
}

COLUMN_NAME_EN = {v: k for k, v in COLUMN_NAME_KR.items()}

TIER_NAMES = ["ğŸ¥‰ ë¸Œë¡ ì¦ˆ ë“±ê¸‰", "ğŸ¥ˆ ì‹¤ë²„ ë“±ê¸‰", "ğŸ¥‡ ê³¨ë“œ ë“±ê¸‰", "ğŸ’ í”Œë˜í‹°ë„˜ ë“±ê¸‰"]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë°ì´í„° ë¡œë”
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
@st.cache_data(show_spinner=False)
def load_data(path: str) -> pd.DataFrame:
    """ë‹¤ì¤‘ ì¸ì½”ë”© í´ë°±ìœ¼ë¡œ CSV/Excel ë¡œë“œ."""
    if not os.path.exists(path):
        raise FileNotFoundError(f"íŒŒì¼ ì—†ìŒ: {path}")

    lower = path.lower()
    if lower.endswith((".xlsx", ".xls")):
        return pd.read_excel(path)

    for enc in ("utf-8-sig", "cp949", "euc-kr", "latin1"):
        try:
            return pd.read_csv(path, encoding=enc, sep=None, engine="python")
        except Exception:
            continue

    return pd.read_csv(path, encoding="utf-8", sep=None, engine="python", errors="replace")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def select_columns(df: pd.DataFrame) -> pd.DataFrame:
    """í•„ìš” ì»¬ëŸ¼ ì„ íƒ."""
    sel = [
        "annual_income", "Recency", "Monetary", "Frequency",
        "num_purchase_store", "num_purchase_web", "num_purchase_discount",
    ]
    return df[sel].copy()

def drop_missing_and_top_outlier(df: pd.DataFrame) -> pd.DataFrame:
    """ê²°ì¸¡ì¹˜ ì œê±° ë° ìµœëŒ€ê°’ ì´ìƒì¹˜ ì œê±°."""
    df = df.dropna().copy()
    max_val = df["annual_income"].max()
    return df[df["annual_income"] < max_val].copy()

def iqr_clean(df: pd.DataFrame) -> pd.DataFrame:
    """IQR ê¸°ë°˜ ì´ìƒì¹˜ ì œê±°."""
    q1, q3 = df.quantile(0.25), df.quantile(0.75)
    iqr = q3 - q1
    mask = ~((df < (q1 - 1.5*iqr)) | (df > (q3 + 1.5*iqr))).any(axis=1)
    return df[mask].reset_index(drop=True)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# KMeans í´ëŸ¬ìŠ¤í„°ë§
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def fit_kmeans_labels(df_num: pd.DataFrame, k: int = CLUSTER_K) -> Tuple[np.ndarray, float]:
    """KMeans í´ëŸ¬ìŠ¤í„°ë§ ë° ì‹¤ë£¨ì—£ ì ìˆ˜."""
    scaler = StandardScaler()
    x_scaled = scaler.fit_transform(df_num.values)
    km = KMeans(n_clusters=k, random_state=SEED, n_init="auto")
    labels = km.fit_predict(x_scaled)
    sil = silhouette_score(x_scaled, labels)
    return labels, sil

def make_tier_mapping(df_with_labels: pd.DataFrame) -> Dict[int, str]:
    """í´ëŸ¬ìŠ¤í„° â†’ ë“±ê¸‰ ë§¤í•‘ (Monetary & Frequency ê¸°ë°˜)."""
    means = (
        df_with_labels.groupby("cluster")[["Monetary", "Frequency"]]
        .mean()
        .assign(score=lambda d: 0.6*d["Monetary"] + 0.4*d["Frequency"])
        .sort_values("score")
    )
    return {int(c): TIER_NAMES[i] for i, c in enumerate(means.index.tolist())}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LGBM í•™ìŠµ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def train_lgbm(features: pd.DataFrame, labels: pd.Series, 
               random_search: bool = False, n_iter: int = 15) -> Tuple[LGBMClassifier, Dict]:
    """LGBM ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ."""
    xtr, xte, ytr, yte = train_test_split(
        features, labels, test_size=0.2, random_state=SEED, stratify=labels
    )

    if random_search:
        base = LGBMClassifier(random_state=SEED, verbose=-1)
        space = {
            "n_estimators": randint(60, 140),
            "learning_rate": uniform(0.01, 0.15),
            "max_depth": randint(3, 12),
            "min_child_samples": randint(10, 80),
            "reg_alpha": uniform(0.0, 0.6),
            "reg_lambda": uniform(0.0, 0.6),
            "num_leaves": randint(7, 48),
        }
        gs = RandomizedSearchCV(base, space, n_iter=n_iter, cv=3, n_jobs=-1, random_state=SEED)
        gs.fit(xtr, ytr)
        best_params = gs.best_params_
        model = LGBMClassifier(random_state=SEED, verbose=-1, **best_params)
    else:
        best_params = {}
        model = LGBMClassifier(random_state=SEED, verbose=-1)

    model.fit(xtr, ytr)

    meta = {
        "train_score": model.score(xtr, ytr),
        "test_score": model.score(xte, yte),
        "best_params": best_params,
        "trained_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    }
    return model, meta

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì‹œê°í™”
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def pca_scatter(df_num: pd.DataFrame, labels: np.ndarray, tier_map: Dict) -> alt.Chart:
    """PCA 2D ì‚°ì ë„."""
    x = StandardScaler().fit_transform(df_num.values)
    xy = PCA(n_components=2, random_state=SEED).fit_transform(x)

    chart_df = pd.DataFrame({
        "x": xy[:, 0],
        "y": xy[:, 1],
        "cluster": labels.astype(int),
        "tier": [tier_map.get(int(c), f"cluster {c}") for c in labels]
    })

    return alt.Chart(chart_df).mark_circle(size=80, opacity=0.8).encode(
        x=alt.X("x:Q", title="ì£¼ìš”ì„±ë¶„ 1"),
        y=alt.Y("y:Q", title="ì£¼ìš”ì„±ë¶„ 2"),
        color=alt.Color("tier:N", title="ë“±ê¸‰"),
        tooltip=["tier:N", "cluster:N", "x:Q", "y:Q"]
    ).properties(height=450, title="ê³ ê° ì„¸ë¶„í™” ë¶„í¬ë„")

def feature_importance_chart(model: LGBMClassifier, feature_cols: list) -> alt.Chart:
    """íŠ¹ì„± ì¤‘ìš”ë„ ì°¨íŠ¸."""
    imp = pd.Series(
        model.feature_importances_,
        index=[COLUMN_NAME_KR.get(c, c) for c in feature_cols]
    ).sort_values(ascending=True)

    return alt.Chart(imp.reset_index().rename(columns={"index": "íŠ¹ì„±", 0: "ì¤‘ìš”ë„"})).mark_barh().encode(
        x="ì¤‘ìš”ë„:Q",
        y=alt.Y("íŠ¹ì„±:N", sort="-x")
    ).properties(height=300, title="íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„")

def cluster_stats(df_labeled: pd.DataFrame, tier_map: Dict) -> pd.DataFrame:
    """ë“±ê¸‰ë³„ í†µê³„."""
    stats = df_labeled.groupby("cluster").agg({
        "annual_income": ["mean", "count"],
        "Monetary": "mean",
        "Frequency": "mean",
    }).round(0)
    stats.columns = ["í‰ê· ì†Œë“(ì›)", "ê³ ê°ìˆ˜", "í‰ê· êµ¬ë§¤ì•¡(ì›)", "í‰ê· êµ¬ë§¤ë¹ˆë„"]
    stats.index = [tier_map.get(i, f"cluster {i}") for i in stats.index]
    return stats

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„¸ì…˜ ì´ˆê¸°í™”
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
if "model_trained" not in st.session_state:
    st.session_state["model_trained"] = False
    st.session_state["model"] = None
    st.session_state["feature_cols"] = None
    st.session_state["tier_map"] = None
    st.session_state["meta"] = None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸ ì•±
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def main() -> None:
    st.title("ğŸ’¼ ê³ ê° ë“±ê¸‰ ì˜ˆì¸¡ ì‹œìŠ¤í…œ")
    st.markdown("KMeans í´ëŸ¬ìŠ¤í„°ë§ + LightGBMìœ¼ë¡œ ê³ ê°ì„ ì„¸ë¶„í™”í•˜ê³  ë“±ê¸‰ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.")

    # ë°ì´í„° ê²½ë¡œ (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’)
    data_path = os.getenv("CUSTOMER_DATA_PATH", "data/customer_data.csv")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        df_raw = load_data(data_path)
    except FileNotFoundError as e:
        st.error(f"âŒ {e}")
        st.info(f"ë°ì´í„° íŒŒì¼ì„ `{data_path}` ê²½ë¡œì— ì €ì¥í•˜ì„¸ìš”.")
        return

    # ì „ì²˜ë¦¬
    df = iqr_clean(drop_missing_and_top_outlier(select_columns(df_raw)))

    if df.empty:
        st.error("âŒ ì „ì²˜ë¦¬ í›„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ ë°ì´í„°ë¥¼ ì ê²€í•˜ì„¸ìš”.")
        return

    # í´ëŸ¬ìŠ¤í„°ë§
    labels, sil = fit_kmeans_labels(df, k=CLUSTER_K)
    df_labeled = df.copy()
    df_labeled["cluster"] = labels
    tier_map = make_tier_mapping(df_labeled)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # íƒ­ êµ¬ì„±
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    tab1, tab2, tab3 = st.tabs(["ğŸ“Š ë°ì´í„° ë¶„ì„", "ğŸ¤– ëª¨ë¸ í•™ìŠµ", "ğŸ¯ ê³ ê° ë“±ê¸‰ ì˜ˆì¸¡"])

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # íƒ­ 1: ë°ì´í„° ë¶„ì„
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with tab1:
        st.header("ê³ ê° ë°ì´í„° ë¶„ì„")

        col1, col2, col3 = st.columns(3)
        col1.metric("ğŸ“ˆ ì´ ê³ ê° ìˆ˜", f"{len(df):,}")
        col2.metric("ğŸ¯ í´ëŸ¬ìŠ¤í„° ìˆ˜", CLUSTER_K)
        col3.metric("â­ ì‹¤ë£¨ì—£ ì ìˆ˜", f"{sil:.3f}")

        st.subheader("ë°ì´í„° ìš”ì•½")
        summary_df = df.copy()
        summary_df.columns = [COLUMN_NAME_KR.get(c, c) for c in summary_df.columns]
        st.dataframe(summary_df.describe().round(0), use_container_width=True)

        st.subheader("ê³ ê° ë¶„í¬ë„ (PCA)")
        st.altair_chart(pca_scatter(df, labels, tier_map), use_container_width=True)

        st.subheader("ë“±ê¸‰ë³„ í†µê³„")
        st.dataframe(cluster_stats(df_labeled, tier_map), use_container_width=True)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # íƒ­ 2: ëª¨ë¸ í•™ìŠµ
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with tab2:
        st.header("LightGBM ëª¨ë¸ í•™ìŠµ")

        col1, col2 = st.columns(2)
        with col1:
            use_search = st.checkbox("ğŸ” í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í™œì„±í™” (ëŠë¦¼)", value=False)
        with col2:
            if use_search:
                n_iter = st.slider("íŠœë‹ ë°˜ë³µ íšŸìˆ˜", 5, 50, 15, 5)
            else:
                n_iter = 0

        if st.button("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘", use_container_width=True, type="primary"):
            progress_bar = st.progress(0)
            status_text = st.empty()

            try:
                status_text.text("ğŸ“¥ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
                progress_bar.progress(10)

                feature_cols = df.columns.tolist()
                X, y = df_labeled[feature_cols], df_labeled["cluster"]

                status_text.text("ğŸ¤– ëª¨ë¸ í•™ìŠµ ì¤‘...")
                progress_bar.progress(30)

                model, meta = train_lgbm(X, y, random_search=use_search, n_iter=n_iter)

                status_text.text("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
                progress_bar.progress(90)

                # ì„¸ì…˜ì— ì €ì¥
                st.session_state["model"] = model
                st.session_state["feature_cols"] = feature_cols
                st.session_state["tier_map"] = tier_map
                st.session_state["meta"] = meta
                st.session_state["model_trained"] = True

                progress_bar.progress(100)
                status_text.text("âœ… í•™ìŠµ ì™„ë£Œ!")

                # ì„±ëŠ¥ ê²°ê³¼
                st.success("ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì˜ˆì¸¡ íƒ­ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                col1, col2 = st.columns(2)
                col1.metric("âœ… í•™ìŠµ ì •í™•ë„", f"{meta['train_score']:.1%}")
                col2.metric("ğŸ§ª í…ŒìŠ¤íŠ¸ ì •í™•ë„", f"{meta['test_score']:.1%}")

                if meta["best_params"]:
                    st.subheader("ğŸ”§ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°")
                    st.json(meta["best_params"])

                st.info(f"í•™ìŠµ ì™„ë£Œ: {meta['trained_at']}")

            except Exception as e:
                st.error(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {e}")

        else:
            st.info("ìœ„ ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•˜ì„¸ìš”.")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # íƒ­ 3: ê³ ê° ë“±ê¸‰ ì˜ˆì¸¡
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with tab3:
        st.header("ê³ ê° ë“±ê¸‰ ì˜ˆì¸¡")

        if not st.session_state["model_trained"]:
            st.warning("âš ï¸ ë¨¼ì € **ëª¨ë¸ í•™ìŠµ** íƒ­ì—ì„œ ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.")
        else:
            st.success("âœ… ëª¨ë¸ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ê³ ê° ì •ë³´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

            # ì…ë ¥ í¼
            st.subheader("ê³ ê° ì •ë³´ ì…ë ¥")

            col1, col2 = st.columns(2)
            col3, col4 = st.columns(2)
            col5, col6 = st.columns(2)
            col7 = st.columns(1)[0]

            inputs: Dict[str, float] = {}

            feature_cols = st.session_state["feature_cols"]

            # ê° íŠ¹ì„±ë³„ ì…ë ¥ ìœ„ì ¯
            inputs_layout = [
                (col1, "annual_income"),
                (col2, "Recency"),
                (col3, "Monetary"),
                (col4, "Frequency"),
                (col5, "num_purchase_store"),
                (col6, "num_purchase_web"),
                (col7, "num_purchase_discount"),
            ]

            for col, col_en in inputs_layout:
                if col_en in feature_cols:
                    vmin = float(df[col_en].min())
                    vmax = float(df[col_en].max())
                    vmean = float(df[col_en].mean())
                    col_kr = COLUMN_NAME_KR.get(col_en, col_en)

                    if col_en.startswith("num_purchase") or col_en in ["Frequency", "Recency"]:
                        inputs[col_en] = col.slider(
                            col_kr,
                            min_value=int(vmin),
                            max_value=int(vmax),
                            value=int(vmean),
                            step=1
                        )
                    else:
                        inputs[col_en] = col.number_input(
                            col_kr,
                            min_value=vmin,
                            value=round(vmean, 0),
                            step=1000.0 if col_en == "annual_income" else 100.0
                        )

            # ì˜ˆì¸¡ ë²„íŠ¼
            if st.button("ğŸ¯ ë“±ê¸‰ ì˜ˆì¸¡í•˜ê¸°", use_container_width=True, type="primary"):
                input_df = pd.DataFrame([inputs], columns=feature_cols)
                pred = int(st.session_state["model"].predict(input_df)[0])
                pred_proba = st.session_state["model"].predict_proba(input_df)[0]

                tier = st.session_state["tier_map"].get(pred, f"cluster {pred}")

                st.balloons()
                st.success(f"# {tier}")

                # í™•ë¥  ë¶„í¬
                st.subheader("ì˜ˆì¸¡ ì‹ ë¢°ë„")
                proba_df = pd.DataFrame({
                    "ë“±ê¸‰": [st.session_state["tier_map"].get(i, f"cluster {i}") for i in range(len(pred_proba))],
                    "í™•ë¥ ": pred_proba
                })
                st.bar_chart(proba_df.set_index("ë“±ê¸‰"))

                # íŠ¹ì„± ì¤‘ìš”ë„
                st.subheader("ì˜ì‚¬ê²°ì • ê·¼ê±° (íŠ¹ì„± ì¤‘ìš”ë„)")
                st.altair_chart(feature_importance_chart(st.session_state["model"], feature_cols), use_container_width=True)

if __name__ == "__main__":
    main()
